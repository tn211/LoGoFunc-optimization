{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from lightgbm import Dataset\n",
    "import lightgbm\n",
    "import utils\n",
    "from utils import *\n",
    "import os, csv, ast\n",
    "import shutil\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The models that this one uses were trained using the original LoGoFunc `train.py` script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25546, 500) (2831, 500)\n"
     ]
    }
   ],
   "source": [
    "# Load both test and reload training data.\n",
    "X_train = pd.read_csv('../data/X_train_500.csv', low_memory=False)\n",
    "X_test = pd.read_csv('../data/X_test_500.csv', low_memory=False)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2831, 472)\n",
      "Test Accuracy: 0.8689509007417874\n",
      "Test Precision: 0.8772429518278922\n",
      "Test Recall: 0.8689509007417874\n",
      "Test F1 Score: 0.8721241510398384\n",
      "Test ROC AUC Score: 0.9334290595243194\n",
      "Confusion Matrix:\n",
      "[[1179   37  123]\n",
      " [  10  105   37]\n",
      " [  97   67 1176]]\n"
     ]
    }
   ],
   "source": [
    "# Define the output path for the CSV file.\n",
    "output_path = '../results/LGBM-validation.csv'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import joblib\n",
    "import utils\n",
    "\n",
    "# This is the soft-voting function used to aggregate the predictions.\n",
    "def soft_vote(preds):\n",
    "    summed_preds = [[np.sum(preds[:, j][:, i]) for i in range(3)] for j in range(len(preds[0]))]\n",
    "    return [softmax(np.log(sp)) for sp in summed_preds]\n",
    "\n",
    "# Again, this checks for columns that only contain NaN values.\n",
    "def drop_allnan(data):\n",
    "    for col in data.columns:\n",
    "        if data[col].isna().sum() == len(data):\n",
    "            data = data.drop(columns=col)\n",
    "    return data\n",
    "\n",
    "# Input the training data and do initial pre-processing.\n",
    "X_train = drop_allnan(X_train)\n",
    "columns = X_train.columns.tolist()\n",
    "\n",
    "# Invoke the pre-processing function.\n",
    "preprocessor = joblib.load('../models/lightgbm/preprocessor.joblib')\n",
    "\n",
    "# Iterate over the models in the ensemble.\n",
    "models = []\n",
    "num_models = 27\n",
    "for i in range(num_models):\n",
    "    models.append(joblib.load(f'../models/lightgbm/model_{i}.joblib'))\n",
    "\n",
    "# Feed the data into the pre-processor functions.\n",
    "y_test = pd.read_csv('../data/y_test_id.csv', low_memory=False)\n",
    "impact_vals = {'LOW': 0, 'MODIFIER': 1, 'MODERATE': 1.5, 'HIGH': 2}\n",
    "encoded_impacts = [impact_vals[imp] for imp in X_test['IMPACT']]\n",
    "X_test = X_test.drop(columns=['IMPACT'])\n",
    "X_test['IMPACT'] = encoded_impacts\n",
    "X_test = X_test[columns]\n",
    "ids = X_test['ID'].tolist()\n",
    "X_test = X_test.drop(columns='ID')\n",
    "\n",
    "# Make sure the data types are the same. because NumPy arrays are not tolerated in places where DataFrames are expected.\n",
    "for col in X_test.columns:\n",
    "    X_test[col] = X_test[col].astype(X_train[col].dtype)\n",
    "X_test = utils.transform(X_test, preprocessor)\n",
    "\n",
    "# Pool the predictions into a list.\n",
    "all_preds = []\n",
    "for i in range(num_models):\n",
    "    preds = models[i].predict(X_test, num_iteration=-1, pred_leaf=False)\n",
    "    all_preds.append(preds)\n",
    "\n",
    "# Apply the soft-voting function.\n",
    "y_pred_proba = soft_vote(np.array(all_preds))\n",
    "y_pred = [np.argmax(p) for p in y_pred_proba]\n",
    "\n",
    "# Map the labels to numbers.\n",
    "label_mapping = {'Neutral': 0, 'GOF': 1, 'LOF': 2}\n",
    "y_test_numeric = [label_mapping[label] for label in y_test['label']]\n",
    "\n",
    "# Perform the evaluation using Scikit-learn's metrics.\n",
    "accuracy = accuracy_score(y_test_numeric, y_pred)\n",
    "precision = precision_score(y_test_numeric, y_pred, average='weighted')\n",
    "recall = recall_score(y_test_numeric, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test_numeric, y_pred, average='weighted')\n",
    "roc_auc = roc_auc_score(y_test_numeric, y_pred_proba, multi_class='ovo')\n",
    "conf_matrix = confusion_matrix(y_test_numeric, y_pred)\n",
    "\n",
    "# Print the recorded metrics.\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "print(f'Test Precision: {precision}')\n",
    "print(f'Test Recall: {recall}')\n",
    "print(f'Test F1 Score: {f1}')\n",
    "print(f'Test ROC AUC Score: {roc_auc}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "\n",
    "out = []\n",
    "for i in range(len(y_pred)):\n",
    "    out.append([ids[i], ['Neutral', 'GOF', 'LOF'][y_pred[i]], *y_pred_proba[i]])\n",
    "out = pd.DataFrame(out, columns=['ID', 'prediction', 'LoGoFunc_Neutral', 'LoGoFunc_GOF', 'LoGoFunc_LOF'])\n",
    "out.to_csv(output_path, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation against y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "metrics_file = '../metrics/LGBM-metrics.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   accuracy  precision    recall  f1_score  \\\n",
      "0  0.868951   0.877243  0.816303  0.872124   \n",
      "\n",
      "                                   confusion_matrix  \n",
      "0  [[1179, 37, 123], [10, 105, 37], [97, 67, 1176]]  \n"
     ]
    }
   ],
   "source": [
    "# This is just to manually verify the test output.\n",
    "predictions_file = output_path\n",
    "\n",
    "# Load y_test.\n",
    "y_test_path = '../data/y_test_id.csv'\n",
    "y_test = pd.read_csv(y_test_path)\n",
    "y_true = y_test['label']\n",
    "\n",
    "# Encode labels to numbers.\n",
    "label_mapping = {'Neutral': 0, 'GOF': 1, 'LOF': 2}\n",
    "y_true_numeric = [label_mapping[label] for label in y_true]\n",
    "\n",
    "# Load predictions.\n",
    "predictions = pd.read_csv(predictions_file)\n",
    "\n",
    "# Ensure DataFrames have same number of rows.\n",
    "assert len(predictions) == len(y_test)\n",
    "\n",
    "# Align predictions and true labels by index\n",
    "y_pred = predictions['prediction']\n",
    "y_pred_numeric = [label_mapping[label] for label in y_pred]\n",
    "\n",
    "# Perform the evaluation using Scikit-learn's metrics.\n",
    "accuracy = accuracy_score(y_true_numeric, y_pred_numeric)\n",
    "precision = precision_score(y_true_numeric, y_pred_numeric, average='weighted')\n",
    "recall = recall_score(y_true_numeric, y_pred_numeric, average='macro')\n",
    "f1 = f1_score(y_true_numeric, y_pred_numeric, average='weighted')\n",
    "conf_matrix = confusion_matrix(y_true_numeric, y_pred_numeric)\n",
    "\n",
    "results = [{\n",
    "    'accuracy': accuracy,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'f1_score': f1,\n",
    "    'confusion_matrix': conf_matrix.tolist()\n",
    "}]\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(metrics_file, index=False)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Macro-REC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../metrics/LGBM-evaluation.csv\n"
     ]
    }
   ],
   "source": [
    "input_file = '../metrics/LGBM-metrics.csv'\n",
    "output_file = '../metrics/LGBM-evaluation.csv'\n",
    "\n",
    "def macro(confusion_matrix):\n",
    "    cm = np.array(confusion_matrix)\n",
    "    recalls = np.diag(cm) / np.sum(cm, axis=1)\n",
    "    return np.mean(recalls)\n",
    "\n",
    "def micro(confusion_matrix):\n",
    "    cm = np.array(confusion_matrix)\n",
    "    true_positives = np.diag(cm)\n",
    "    total_true_positives = np.sum(true_positives)\n",
    "    total_actual_positives = np.sum(cm)\n",
    "    return total_true_positives / total_actual_positives\n",
    "\n",
    "with open(input_file, 'r') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    rows = list(reader)\n",
    "\n",
    "for row in rows:\n",
    "    confusion_matrix = ast.literal_eval(row['confusion_matrix'])\n",
    "    macro = macro(confusion_matrix)\n",
    "    micro_recall = micro(confusion_matrix)\n",
    "    row['micro_recall'] = f'{micro_recall:.4f}'\n",
    "    row['macro'] = f'{macro:.4f}'\n",
    "\n",
    "    # Remove the original 'weighted' REC column.\n",
    "    if 'recall' in row:\n",
    "        del row['recall']\n",
    "\n",
    "with open(output_file, 'w', newline='') as csvfile:\n",
    "    fieldnames = ['accuracy', 'precision', 'f1_score', 'micro_recall', 'macro', 'confusion_matrix']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in rows:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logofunc2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
