{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, ast, json, csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from tqdm.notebook import tqdm\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report, matthews_corrcoef\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import  RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEGONE_FEATURES: ['feature369', 'feature265', 'feature473', 'feature420', 'feature461', 'feature415', 'feature373', 'feature433', 'feature395', 'feature443', 'feature242', 'feature258', 'feature451', 'feature366', 'feature425', 'feature46', 'feature31', 'feature6', 'feature89', 'feature52', 'feature468', 'feature286', 'feature315', 'feature191', 'feature196', 'feature91', 'feature192', 'feature27', 'feature20', 'feature111', 'feature100', 'feature90', 'feature72', 'feature255', 'feature298', 'feature50', 'feature67', 'feature136', 'feature59', 'feature92', 'feature139', 'feature60', 'feature48', 'feature145', 'feature78', 'feature115', 'feature372', 'feature472', 'feature439', 'feature382', 'feature475', 'feature353', 'feature134', 'feature132', 'feature35', 'feature42', 'feature96', 'feature249', 'feature82', 'feature18', 'feature23', 'feature123', 'feature99', 'feature32', 'feature70', 'feature68', 'feature178', 'feature401', 'feature141', 'feature294', 'feature447', 'feature95', 'feature375', 'feature55', 'feature421', 'feature56', 'feature34', 'feature304', 'feature77', 'feature435', 'feature365', 'feature226', 'feature360', 'feature347', 'feature193', 'feature314', 'feature299', 'feature403', 'feature402', 'feature334', 'feature350', 'feature110', 'feature271', 'feature229', 'feature337', 'feature268', 'feature328', 'feature293', 'feature223', 'feature85', 'feature361', 'feature140', 'feature297', 'feature254', 'feature340', 'feature306', 'feature172', 'feature155', 'feature240', 'feature236', 'feature239', 'feature209', 'feature238', 'feature187', 'feature364', 'feature330', 'feature437', 'feature159', 'feature441', 'feature431', 'feature396', 'feature358', 'feature295', 'feature444', 'feature316', 'feature380', 'feature75', 'feature448', 'feature205', 'feature430', 'feature148', 'feature335', 'feature326', 'feature308', 'feature320', 'feature318', 'feature389', 'feature276', 'feature305', 'feature296', 'feature390', 'feature204', 'feature127', 'feature54', 'feature408', 'feature376', 'feature413', 'feature282', 'feature338', 'feature356', 'feature385', 'feature283', 'feature245', 'feature93', 'feature362', 'feature352', 'feature264', 'feature272', 'feature333', 'feature384', 'feature86', 'feature302', 'feature429', 'feature231', 'feature152', 'feature174', 'feature349', 'feature322', 'feature379', 'feature300', 'feature252', 'feature464', 'feature200', 'feature164', 'feature194', 'feature197', 'feature218', 'feature368', 'feature423', 'feature339', 'feature387', 'feature374', 'feature391', 'feature181', 'feature311', 'feature427', 'feature213', 'feature125', 'feature184', 'feature290', 'feature284', 'feature317', 'feature406', 'feature237', 'feature281', 'feature449', 'feature310', 'feature248', 'feature177', 'feature359', 'feature394', 'feature291', 'feature416', 'feature371', 'feature327', 'feature336', 'feature355', 'feature312', 'feature182', 'feature367', 'feature325', 'feature203', 'feature263', 'feature412', 'feature214', 'feature219', 'feature377', 'feature138', 'feature79', 'feature109', 'feature195', 'feature454', 'feature405', 'feature201', 'feature463', 'feature432', 'feature210', 'feature465', 'feature484', 'feature397', 'feature381', 'feature456', 'feature462', 'feature288', 'feature438', 'feature275', 'feature383', 'feature363', 'feature212', 'feature158', 'feature392', 'feature445', 'feature498', 'feature494', 'feature492', 'feature478', 'feature493', 'feature469', 'feature489', 'feature188', 'feature168', 'feature418', 'feature118', 'feature404', 'feature167', 'feature113', 'feature142', 'feature107', 'feature147', 'feature157', 'feature257', 'feature103', 'feature114', 'feature94', 'feature189', 'feature112', 'feature76', 'feature260', 'feature161', 'feature230', 'feature143', 'feature83', 'feature175', 'feature162', 'feature105', 'feature58', 'feature227', 'feature73', 'feature228', 'feature222', 'feature351', 'feature388', 'feature126', 'feature206', 'feature440', 'feature292', 'feature146', 'feature341', 'feature303', 'feature307', 'feature321', 'feature243', 'feature287', 'feature149', 'feature128', 'feature51', 'feature69', 'feature190', 'feature221', 'feature428', 'feature124', 'feature133', 'feature235', 'feature344', 'feature217', 'feature483', 'feature487', 'feature477', 'feature488', 'feature474', 'feature453', 'feature471', 'feature499', 'feature496', 'feature497', 'feature495', 'feature393', 'feature476', 'feature479', 'feature470', 'feature457', 'feature485', 'feature446', 'feature486', 'feature481', 'feature490', 'feature480', 'feature491', 'feature323', 'feature399', 'feature244', 'feature482', 'feature400', 'feature466', 'feature436', 'feature411', 'feature121', 'feature153', 'feature173', 'feature80', 'feature154', 'feature62', 'feature150', 'feature186', 'feature279', 'feature57', 'feature280', 'feature176', 'feature116', 'feature269', 'feature37', 'feature65', 'feature108', 'feature250', 'feature259', 'feature53', 'feature64', 'feature234', 'feature39', 'feature104', 'feature97', 'feature165', 'feature313', 'feature459', 'feature442', 'feature261', 'feature407', 'feature342', 'feature169', 'feature151', 'feature33', 'feature117', 'feature266', 'feature426', 'feature331', 'feature424', 'feature370', 'feature422', 'feature343', 'feature285', 'feature357', 'feature274', 'feature256', 'feature409', 'feature267', 'feature417', 'feature273', 'feature233']\n",
      "MEDIAN_FEATURES: ['feature40', 'feature84', 'feature378', 'feature156', 'feature458', 'feature253', 'feature120', 'feature38', 'feature87', 'feature171', 'feature199', 'feature246', 'feature324', 'feature220', 'feature467', 'feature47', 'feature170', 'feature12', 'feature15', 'feature22', 'feature49', 'feature26', 'feature61', 'feature130', 'feature4', 'feature13', 'feature45', 'feature289', 'feature24', 'feature43', 'feature329', 'feature11', 'feature348', 'feature332', 'feature21', 'feature144', 'feature180', 'feature232', 'feature10', 'feature25', 'feature160', 'feature119', 'feature224', 'feature3', 'feature66', 'feature309', 'feature241', 'feature102', 'feature19', 'feature9', 'feature7', 'feature225', 'feature8', 'feature5', 'feature131', 'feature455', 'feature354', 'feature452', 'feature460', 'feature247', 'feature101', 'feature88', 'feature17', 'feature319', 'feature216', 'feature30', 'feature44', 'feature135', 'feature208', 'feature278', 'feature198', 'feature166', 'feature202', 'feature16', 'feature129', 'feature137', 'feature179', 'feature211', 'feature71', 'feature106', 'feature122', 'feature215', 'feature270', 'feature207', 'feature81', 'feature450', 'feature41', 'feature14', 'feature345', 'feature163', 'feature63', 'feature98', 'feature36', 'feature346', 'feature185', 'feature183', 'feature251', 'feature386', 'feature74', 'feature28']\n"
     ]
    }
   ],
   "source": [
    "# Load encoded NEGONE and MEDIAN feature lists.\n",
    "file_path = '../data/negone_median.json'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "negone_features = data.get('NEGONE_FEATURES', [])\n",
    "median_features = data.get('MEDIAN_FEATURES', [])\n",
    "\n",
    "print('NEGONE_FEATURES:', negone_features)\n",
    "print('MEDIAN_FEATURES:', median_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Keys: [6, 18, 23, 32, 42, 48, 50, 59, 60, 67, 68, 70, 78, 82, 89, 92, 96, 99, 115, 123, 136, 139, 145, 178, 249, 298]\n",
      "Length: 26\n"
     ]
    }
   ],
   "source": [
    "# Load other encoded feature names. \n",
    "\n",
    "file_path = '../data/patterns.json'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Get keys, convert to integer numbers.\n",
    "keys_list = [int(key.replace('feature', '')) for key in data.keys()]\n",
    "drop_numbers = keys_list\n",
    "print('Extracted Keys:', drop_numbers)\n",
    "print('Length:', len(drop_numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveBeforeAfterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.drop_cols = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print(X.shape)\n",
    "        self.drop_cols = [col for col in X.columns if any(f'feature{num}' == col for num in drop_numbers)]\n",
    "        print(f\"Columns to be dropped: {self.drop_cols}\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print(f\"Columns being dropped: {[col for col in self.drop_cols if col in X.columns]}\")\n",
    "        X = X.drop(columns=self.drop_cols, errors='ignore')\n",
    "        print(X.shape)\n",
    "        return X\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return [f for f in input_features if f not in self.drop_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(preprocessor, train_data, train_labels, quiet=False):\n",
    "    for k, v in preprocessor.steps:\n",
    "        print(f\"Applying step: {k}\")\n",
    "        print(f\"Input shape: {train_data.shape}\")\n",
    "        print(f\"Input columns: {train_data.columns}\")\n",
    "        \n",
    "        if k == 'initial':\n",
    "            start = time.time()\n",
    "            v.fit(train_data)\n",
    "            train_data = pd.DataFrame(v.transform(train_data), columns=v.get_feature_names_out())\n",
    "            end = time.time()\n",
    "            if not quiet:\n",
    "                print(k + ' took ' + str(end - start) + ' to run.')\n",
    "        elif k == 'oversampling' or k == 'undersampling':\n",
    "            start = time.time()\n",
    "            input_features = train_data.columns\n",
    "            train_data, train_labels = v.fit_resample(train_data, train_labels)\n",
    "            train_data = pd.DataFrame(train_data, columns=input_features)\n",
    "            end = time.time()\n",
    "            if not quiet:\n",
    "                print(k + ' took ' + str(end - start) + ' to run.')\n",
    "        else:\n",
    "            start = time.time()\n",
    "            v.fit(train_data)\n",
    "            input_features = train_data.columns\n",
    "            train_data = pd.DataFrame(v.transform(train_data), columns=v.get_feature_names_out(input_features))\n",
    "            end = time.time()\n",
    "            if not quiet:\n",
    "                print(k + ' took ' + str(end - start) + ' to run.')\n",
    "        \n",
    "        print(f\"Output shape: {train_data.shape}\")\n",
    "        print(f\"Output columns: {train_data.columns}\")\n",
    "        print(\"---\")\n",
    "\n",
    "    for col in train_data.columns:\n",
    "        try:\n",
    "            train_data[col] = train_data[col].astype('float')\n",
    "        except:\n",
    "            train_data[col] = train_data[col].astype('category')\n",
    "\n",
    "    return train_data, train_labels\n",
    "\n",
    "def transform(test_data, preprocessor, quiet=False):\n",
    "    for k, v in preprocessor.steps:\n",
    "        if k == 'initial':\n",
    "            test_data = pd.DataFrame(v.transform(test_data), columns=v.get_feature_names_out())\n",
    "        elif k == 'oversampling' or k == 'undersampling':\n",
    "            continue\n",
    "        else:\n",
    "            test_data = v.transform(test_data)\n",
    "    test_data = pd.DataFrame(test_data)\n",
    "\n",
    "    for col in test_data.columns:\n",
    "        try:\n",
    "            test_data[col] = test_data[col].astype('float')\n",
    "        except:\n",
    "            test_data[col] = test_data[col].astype('category')\n",
    "\n",
    "    return test_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the soft-voting function used to aggregate the predictions.\n",
    "def soft_vote(preds):\n",
    "    summed_preds = [[np.sum(preds[:, j][:, i]) for i in range(3)] for j in range(len(preds[0]))]\n",
    "    return [softmax(np.log(sp)) for sp in summed_preds]\n",
    "\n",
    "# This is used to drop columns that only contain NaN values.\n",
    "def drop_allnan(data):\n",
    "    for col in data.columns:\n",
    "        if data[col].isna().sum() == len(data):\n",
    "            data = data.drop(columns=col)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing model 256\n",
      "Shape of data: (2831, 257)\n",
      "Shape of X_train: (25546, 257)\n",
      "Filtered NEGONE_FEATURES: ['feature242', 'feature46', 'feature31', 'feature6', 'feature89', 'feature52', 'feature191', 'feature196', 'feature91', 'feature192', 'feature27', 'feature20', 'feature111', 'feature100', 'feature90', 'feature72', 'feature255', 'feature50', 'feature67', 'feature136', 'feature59', 'feature92', 'feature139', 'feature60', 'feature48', 'feature145', 'feature78', 'feature115', 'feature134', 'feature132', 'feature35', 'feature42', 'feature96', 'feature249', 'feature82', 'feature18', 'feature23', 'feature123', 'feature99', 'feature32', 'feature70', 'feature68', 'feature178', 'feature141', 'feature95', 'feature55', 'feature56', 'feature34', 'feature77', 'feature226', 'feature193', 'feature110', 'feature229', 'feature223', 'feature85', 'feature140', 'feature254', 'feature172', 'feature155', 'feature240', 'feature236', 'feature239', 'feature209', 'feature238', 'feature187', 'feature159', 'feature75', 'feature205', 'feature148', 'feature204', 'feature127', 'feature54', 'feature245', 'feature93', 'feature86', 'feature231', 'feature152', 'feature174', 'feature252', 'feature200', 'feature164', 'feature194', 'feature197', 'feature218', 'feature181', 'feature213', 'feature125', 'feature184', 'feature237', 'feature248', 'feature177', 'feature182', 'feature203', 'feature214', 'feature219', 'feature138', 'feature79', 'feature109', 'feature195', 'feature201', 'feature210', 'feature212', 'feature158', 'feature188', 'feature168', 'feature118', 'feature167', 'feature113', 'feature142', 'feature107', 'feature147', 'feature157', 'feature103', 'feature114', 'feature94', 'feature189', 'feature112', 'feature76', 'feature161', 'feature230', 'feature143', 'feature83', 'feature175', 'feature162', 'feature105', 'feature58', 'feature227', 'feature73', 'feature228', 'feature222', 'feature126', 'feature206', 'feature146', 'feature243', 'feature149', 'feature128', 'feature51', 'feature69', 'feature190', 'feature221', 'feature124', 'feature133', 'feature235', 'feature217', 'feature244', 'feature121', 'feature153', 'feature173', 'feature80', 'feature154', 'feature62', 'feature150', 'feature186', 'feature57', 'feature176', 'feature116', 'feature37', 'feature65', 'feature108', 'feature250', 'feature53', 'feature64', 'feature234', 'feature39', 'feature104', 'feature97', 'feature165', 'feature169', 'feature151', 'feature33', 'feature117', 'feature256', 'feature233']\n",
      "Filtered MEDIAN_FEATURES: ['feature40', 'feature84', 'feature156', 'feature253', 'feature120', 'feature38', 'feature87', 'feature171', 'feature199', 'feature246', 'feature220', 'feature47', 'feature170', 'feature12', 'feature15', 'feature22', 'feature49', 'feature26', 'feature61', 'feature130', 'feature4', 'feature13', 'feature45', 'feature24', 'feature43', 'feature11', 'feature21', 'feature144', 'feature180', 'feature232', 'feature10', 'feature25', 'feature160', 'feature119', 'feature224', 'feature3', 'feature66', 'feature241', 'feature102', 'feature19', 'feature9', 'feature7', 'feature225', 'feature8', 'feature5', 'feature131', 'feature247', 'feature101', 'feature88', 'feature17', 'feature216', 'feature30', 'feature44', 'feature135', 'feature208', 'feature198', 'feature166', 'feature202', 'feature16', 'feature129', 'feature137', 'feature179', 'feature211', 'feature71', 'feature106', 'feature122', 'feature215', 'feature207', 'feature81', 'feature41', 'feature14', 'feature163', 'feature63', 'feature98', 'feature36', 'feature185', 'feature183', 'feature251', 'feature74', 'feature28']\n",
      "['feature0', 'feature1', 'feature2', 'feature3', 'feature4', 'feature5', 'feature6', 'feature7', 'feature8', 'feature9', 'feature10', 'feature11', 'feature12', 'feature13', 'feature14', 'feature15', 'feature16', 'feature17', 'feature18', 'feature19', 'feature20', 'feature21', 'feature22', 'feature23', 'feature24', 'feature25', 'feature26', 'feature27', 'feature28', 'feature29', 'feature30', 'feature31', 'feature32', 'feature33', 'feature34', 'feature35', 'feature36', 'feature37', 'feature38', 'feature39', 'feature40', 'feature41', 'feature42', 'feature43', 'feature44', 'feature45', 'feature46', 'feature47', 'feature48', 'feature49', 'feature50', 'feature51', 'feature52', 'feature53', 'feature54', 'feature55', 'feature56', 'feature57', 'feature58', 'feature59', 'feature60', 'feature61', 'feature62', 'feature63', 'feature64', 'feature65', 'feature66', 'feature67', 'feature68', 'feature69', 'feature70', 'feature71', 'feature72', 'feature73', 'feature74', 'feature75', 'feature76', 'feature77', 'feature78', 'feature79', 'feature80', 'feature81', 'feature82', 'feature83', 'feature84', 'feature85', 'feature86', 'feature87', 'feature88', 'feature89', 'feature90', 'feature91', 'feature92', 'feature93', 'feature94', 'feature95', 'feature96', 'feature97', 'feature98', 'feature99', 'feature100', 'feature101', 'feature102', 'feature103', 'feature104', 'feature105', 'feature106', 'feature107', 'feature108', 'feature109', 'feature110', 'feature111', 'feature112', 'feature113', 'feature114', 'feature115', 'feature116', 'feature117', 'feature118', 'feature119', 'feature120', 'feature121', 'feature122', 'feature123', 'feature124', 'feature125', 'feature126', 'feature127', 'feature128', 'feature129', 'feature130', 'feature131', 'feature132', 'feature133', 'feature134', 'feature135', 'feature136', 'feature137', 'feature138', 'feature139', 'feature140', 'feature141', 'feature142', 'feature143', 'feature144', 'feature145', 'feature146', 'feature147', 'feature148', 'feature149', 'feature150', 'feature151', 'feature152', 'feature153', 'feature154', 'feature155', 'feature156', 'feature157', 'feature158', 'feature159', 'feature160', 'feature161', 'feature162', 'feature163', 'feature164', 'feature165', 'feature166', 'feature167', 'feature168', 'feature169', 'feature170', 'feature171', 'feature172', 'feature173', 'feature174', 'feature175', 'feature176', 'feature177', 'feature178', 'feature179', 'feature180', 'feature181', 'feature182', 'feature183', 'feature184', 'feature185', 'feature186', 'feature187', 'feature188', 'feature189', 'feature190', 'feature191', 'feature192', 'feature193', 'feature194', 'feature195', 'feature196', 'feature197', 'feature198', 'feature199', 'feature200', 'feature201', 'feature202', 'feature203', 'feature204', 'feature205', 'feature206', 'feature207', 'feature208', 'feature209', 'feature210', 'feature211', 'feature212', 'feature213', 'feature214', 'feature215', 'feature216', 'feature217', 'feature218', 'feature219', 'feature220', 'feature221', 'feature222', 'feature223', 'feature224', 'feature225', 'feature226', 'feature227', 'feature228', 'feature229', 'feature230', 'feature231', 'feature232', 'feature233', 'feature234', 'feature235', 'feature236', 'feature237', 'feature238', 'feature239', 'feature240', 'feature241', 'feature242', 'feature243', 'feature244', 'feature245', 'feature246', 'feature247', 'feature248', 'feature249', 'feature250', 'feature251', 'feature252', 'feature253', 'feature254', 'feature255', 'feature256']\n",
      "Columns being dropped: []\n",
      "(2831, 255)\n",
      "Per-class MCC:\n",
      "Neutral: 0.8590\n",
      "GOF: 0.4411\n",
      "LOF: 0.7835\n",
      "Overall MCC: 0.7936\n",
      "Model 256 - Test Accuracy: 0.8883786647827623\n",
      "Model 256 - Test Precision: 0.8857401788440746\n",
      "Model 256 - Test Recall: 0.8883786647827623\n",
      "Model 256 - Test F1 Score: 0.8798900689649398\n",
      "Model 256 - Test ROC AUC Score: 0.9316638251247985\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Neutral       0.93      0.92      0.93      1339\n",
      "         GOF       0.77      0.27      0.40       152\n",
      "         LOF       0.86      0.92      0.89      1340\n",
      "\n",
      "    accuracy                           0.89      2831\n",
      "   macro avg       0.85      0.71      0.74      2831\n",
      "weighted avg       0.89      0.89      0.88      2831\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Neutral       0.93      0.92      0.93      1339\n",
      "         GOF       0.77      0.27      0.40       152\n",
      "         LOF       0.86      0.92      0.89      1340\n",
      "\n",
      "    accuracy                           0.89      2831\n",
      "   macro avg       0.85      0.71      0.74      2831\n",
      "weighted avg       0.89      0.89      0.88      2831\n",
      "\n",
      "\n",
      "Per-class MCC:\n",
      "Neutral: 0.8590\n",
      "GOF: 0.4411\n",
      "LOF: 0.7835\n",
      "\n",
      "Metrics for model 256 saved to ../XGBoost/metrics/individual/xgb_df256_metrics.csv\n",
      "Report for model 256 saved to ../XGBoost/metrics/report/xgb_df256_report.csv\n",
      "Confusion Matrix for model 256 saved to ../XGBoost/metrics/matrix/xgb_df256_matrix.csv\n",
      "Results for model 256 saved to ../XGBoost/results/xgb_df256.csv\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Iterate through the entire testing script for each subeset DataFrame in the dictionary. \"\"\"\n",
    "\n",
    "for number in range(1, 500):  \n",
    "    print(f\"Processing model {number}\")\n",
    "    \n",
    "    # Load both datasets.\n",
    "    data = pd.read_csv(f'../data/encoded/test/X_test_{number}_encoded.csv', low_memory=False)\n",
    "    print(f'Shape of data: {data.shape}')\n",
    "    X_train = pd.read_csv(f'../data/encoded/train/X_train_{number}_encoded.csv', low_memory=False)\n",
    "    print(f'Shape of X_train: {X_train.shape}')\n",
    "\n",
    "    model_name = f'xgb_df{number}'\n",
    "    model_path = f'../XGBoost/models/{model_name}'\n",
    "    output_path = f'../XGBoost/results/{model_name}.csv'\n",
    "    metrics_path = f'../XGBoost/metrics/individual/{model_name}_metrics.csv'\n",
    "    report_path = f'../XGBoost/metrics/report/{model_name}_report.csv'\n",
    "    matrix_path = f'../XGBoost/metrics/matrix/{model_name}_matrix.csv'\n",
    "\n",
    "    # These lists are used to determine imputation strategies for the specified features (inherited from LoGoFunc)\n",
    "    NEGONE_FEATURES = ['feature369', 'feature265', 'feature473', 'feature420', 'feature461', 'feature415', 'feature373', 'feature433', 'feature395', 'feature443', 'feature242', 'feature258', 'feature451', 'feature366', 'feature425', 'feature46', 'feature31', 'feature6', 'feature89', 'feature52', 'feature468', 'feature286', 'feature315', 'feature191', 'feature196', 'feature91', 'feature192', 'feature27', 'feature20', 'feature111', 'feature100', 'feature90', 'feature72', 'feature255', 'feature298', 'feature50', 'feature67', 'feature136', 'feature59', 'feature92', 'feature139', 'feature60', 'feature48', 'feature145', 'feature78', 'feature115', 'feature372', 'feature472', 'feature439', 'feature382', 'feature475', 'feature353', 'feature134', 'feature132', 'feature35', 'feature42', 'feature96', 'feature249', 'feature82', 'feature18', 'feature23', 'feature123', 'feature99', 'feature32', 'feature70', 'feature68', 'feature178', 'feature401', 'feature141', 'feature294', 'feature447', 'feature95', 'feature375', 'feature55', 'feature421', 'feature56', 'feature34', 'feature304', 'feature77', 'feature435', 'feature365', 'feature226', 'feature360', 'feature347', 'feature193', 'feature314', 'feature299', 'feature403', 'feature402', 'feature334', 'feature350', 'feature110', 'feature271', 'feature229', 'feature337', 'feature268', 'feature328', 'feature293', 'feature223', 'feature85', 'feature361', 'feature140', 'feature297', 'feature254', 'feature340', 'feature306', 'feature172', 'feature155', 'feature240', 'feature236', 'feature239', 'feature209', 'feature238', 'feature187', 'feature364', 'feature330', 'feature437', 'feature159', 'feature441', 'feature431', 'feature396', 'feature358', 'feature295', 'feature444', 'feature316', 'feature380', 'feature75', 'feature448', 'feature205', 'feature430', 'feature148', 'feature335', 'feature326', 'feature308', 'feature320', 'feature318', 'feature389', 'feature276', 'feature305', 'feature296', 'feature390', 'feature204', 'feature127', 'feature54', 'feature408', 'feature376', 'feature413', 'feature282', 'feature338', 'feature356', 'feature385', 'feature283', 'feature245', 'feature93', 'feature362', 'feature352', 'feature264', 'feature272', 'feature333', 'feature384', 'feature86', 'feature302', 'feature429', 'feature231', 'feature152', 'feature174', 'feature349', 'feature322', 'feature379', 'feature300', 'feature252', 'feature464', 'feature200', 'feature164', 'feature194', 'feature197', 'feature218', 'feature368', 'feature423', 'feature339', 'feature387', 'feature374', 'feature391', 'feature181', 'feature311', 'feature427', 'feature213', 'feature125', 'feature184', 'feature290', 'feature284', 'feature317', 'feature406', 'feature237', 'feature281', 'feature449', 'feature310', 'feature248', 'feature177', 'feature359', 'feature394', 'feature291', 'feature416', 'feature371', 'feature327', 'feature336', 'feature355', 'feature312', 'feature182', 'feature367', 'feature325', 'feature203', 'feature263', 'feature412', 'feature214', 'feature219', 'feature377', 'feature138', 'feature79', 'feature109', 'feature195', 'feature454', 'feature405', 'feature201', 'feature463', 'feature432', 'feature210', 'feature465', 'feature484', 'feature397', 'feature381', 'feature456', 'feature462', 'feature288', 'feature438', 'feature275', 'feature383', 'feature363', 'feature212', 'feature158', 'feature392', 'feature445', 'feature498', 'feature494', 'feature492', 'feature478', 'feature493', 'feature469', 'feature489', 'feature188', 'feature168', 'feature418', 'feature118', 'feature404', 'feature167', 'feature113', 'feature142', 'feature107', 'feature147', 'feature157', 'feature257', 'feature103', 'feature114', 'feature94', 'feature189', 'feature112', 'feature76', 'feature260', 'feature161', 'feature230', 'feature143', 'feature83', 'feature175', 'feature162', 'feature105', 'feature58', 'feature227', 'feature73', 'feature228', 'feature222', 'feature351', 'feature388', 'feature126', 'feature206', 'feature440', 'feature292', 'feature146', 'feature341', 'feature303', 'feature307', 'feature321', 'feature243', 'feature287', 'feature149', 'feature128', 'feature51', 'feature69', 'feature190', 'feature221', 'feature428', 'feature124', 'feature133', 'feature235', 'feature344', 'feature217', 'feature483', 'feature487', 'feature477', 'feature488', 'feature474', 'feature453', 'feature471', 'feature499', 'feature496', 'feature497', 'feature495', 'feature393', 'feature476', 'feature479', 'feature470', 'feature457', 'feature485', 'feature446', 'feature486', 'feature481', 'feature490', 'feature480', 'feature491', 'feature323', 'feature399', 'feature244', 'feature482', 'feature400', 'feature466', 'feature436', 'feature411', 'feature121', 'feature153', 'feature173', 'feature80', 'feature154', 'feature62', 'feature150', 'feature186', 'feature279', 'feature57', 'feature280', 'feature176', 'feature116', 'feature269', 'feature37', 'feature65', 'feature108', 'feature250', 'feature259', 'feature53', 'feature64', 'feature234', 'feature39', 'feature104', 'feature97', 'feature165', 'feature313', 'feature459', 'feature442', 'feature261', 'feature407', 'feature342', 'feature169', 'feature151', 'feature33', 'feature117', 'feature266', 'feature426', 'feature331', 'feature424', 'feature370', 'feature422', 'feature343', 'feature285', 'feature357', 'feature274', 'feature256', 'feature409', 'feature267', 'feature417', 'feature273', 'feature233']\n",
    "    MEDIAN_FEATURES = ['feature40', 'feature84', 'feature378', 'feature156', 'feature458', 'feature253', 'feature120', 'feature38', 'feature87', 'feature171', 'feature199', 'feature246', 'feature324', 'feature220', 'feature467', 'feature47', 'feature170', 'feature12', 'feature15', 'feature22', 'feature49', 'feature26', 'feature61', 'feature130', 'feature4', 'feature13', 'feature45', 'feature289', 'feature24', 'feature43', 'feature329', 'feature11', 'feature348', 'feature332', 'feature21', 'feature144', 'feature180', 'feature232', 'feature10', 'feature25', 'feature160', 'feature119', 'feature224', 'feature3', 'feature66', 'feature309', 'feature241', 'feature102', 'feature19', 'feature9', 'feature7', 'feature225', 'feature8', 'feature5', 'feature131', 'feature455', 'feature354', 'feature452', 'feature460', 'feature247', 'feature101', 'feature88', 'feature17', 'feature319', 'feature216', 'feature30', 'feature44', 'feature135', 'feature208', 'feature278', 'feature198', 'feature166', 'feature202', 'feature16', 'feature129', 'feature137', 'feature179', 'feature211', 'feature71', 'feature106', 'feature122', 'feature215', 'feature270', 'feature207', 'feature81', 'feature450', 'feature41', 'feature14', 'feature345', 'feature163', 'feature63', 'feature98', 'feature36', 'feature346', 'feature185', 'feature183', 'feature251', 'feature386', 'feature74', 'feature28']\n",
    "\n",
    "    # Filter NEGONE_FEATURES and MEDIAN_FEATURES based on current X_train columns.\n",
    "    negone_features_filtered = [feature for feature in NEGONE_FEATURES if feature in X_train.columns]\n",
    "    median_features_filtered = [feature for feature in MEDIAN_FEATURES if feature in X_train.columns]\n",
    "\n",
    "    # Redefine NEGONE_FEATURES and MEDIAN_FEATURES with updated values.\n",
    "    NEGONE_FEATURES = negone_features_filtered\n",
    "    MEDIAN_FEATURES = median_features_filtered\n",
    "\n",
    "    print(\"Filtered NEGONE_FEATURES:\", NEGONE_FEATURES)\n",
    "    print(\"Filtered MEDIAN_FEATURES:\", MEDIAN_FEATURES)\n",
    "\n",
    "    def generate_preprocessor(numeric_features, categorical_features, N_JOBS, cat_encode_type, \n",
    "                            do_specificimpute, do_featureselection, \n",
    "                            do_sampling, do_pca, var_thresh, oversample_technique, \n",
    "                            negone_features=NEGONE_FEATURES, median_features=MEDIAN_FEATURES,\n",
    "                            prefix='', do_feature_subset=False, max_features=1, do_removeppi=False, do_removegtex=False):\n",
    "        cat_encoders = [OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1, encoded_missing_value=-1), \n",
    "                        OneHotEncoder(sparse=False, handle_unknown='infrequent_if_exist', min_frequency=10)]\n",
    "        categorical_transformer = cat_encoders[cat_encode_type]\n",
    "\n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', MinMaxScaler(feature_range =(0, 1), clip=True))])\n",
    "\n",
    "        median_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', MinMaxScaler(feature_range =(0, 1), clip=True))])\n",
    "\n",
    "        negone_transformer = Pipeline(steps=[\n",
    "            ('scaler', MinMaxScaler(feature_range =(0, 1), clip=True)),\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value=-1)),\n",
    "        ])\n",
    "\n",
    "        preprocessor = None\n",
    "        if do_specificimpute:\n",
    "            preprocessor = ColumnTransformer(\n",
    "                transformers=[\n",
    "                    ('median', median_transformer, median_features),\n",
    "                    ('negone', negone_transformer, negone_features),\n",
    "                    ('cat', categorical_transformer, categorical_features),\n",
    "            ])\n",
    "        else:\n",
    "            preprocessor = ColumnTransformer(\n",
    "                transformers=[\n",
    "                    ('numeric', numeric_transformer, numeric_features),\n",
    "                    ('cat', categorical_transformer, categorical_features),\n",
    "                ])\n",
    "\n",
    "        vt = VarianceThreshold(threshold=var_thresh)\n",
    "        steps = [('initial', preprocessor), ('removeba', RemoveBeforeAfterTransformer()), ('variance_threshold', vt)]\n",
    "        if do_sampling == 1:\n",
    "            steps.append(('undersampling', RandomUnderSampler(random_state=42)))\n",
    "        if do_sampling == 2:\n",
    "            oversamplers = [SMOTE(n_jobs=N_JOBS,random_state=42), RandomOverSampler(random_state=42)]\n",
    "            steps.append(('oversampling', oversamplers[oversample_technique]))\n",
    "        if do_pca:\n",
    "            steps.append(('pca', PCA()))\n",
    "\n",
    "        preprocessor = Pipeline(steps=steps)\n",
    "        return preprocessor\n",
    "\n",
    "    X_test = data\n",
    "    y_test = pd.read_csv('../data/y_test_id.csv', low_memory=False)\n",
    "    model_type = 'xgb'\n",
    "    num_models = 27\n",
    "\n",
    "    features = X_test.columns.tolist()\n",
    "    print(features)\n",
    "\n",
    "    # Load training data and extract columns (features).\n",
    "    X_train = drop_allnan(X_train)\n",
    "    columns = X_train.columns.tolist()\n",
    "\n",
    "    # Pre-processor generation with multiple parameters that can be tweaked. I used the same values as LoGoFunc\n",
    "    preprocessor = joblib.load(f'{model_path}/preprocessor.joblib')\n",
    "\n",
    "    # Iterate over the models in the ensemble.\n",
    "    models = []\n",
    "    num_models = 27  \n",
    "    for i in range(num_models):\n",
    "        models.append(joblib.load(f'{model_path}/{model_type}_model_{i}.joblib'))\n",
    "\n",
    "    # This encodes the IMPACT feature's column, then drops the ID column.\n",
    "    if 'feature2' in features:\n",
    "        feature2_vals = {'LOW': 0, 'MODIFIER': 1, 'MODERATE': 1.5, 'HIGH': 2}\n",
    "        encoded_feature2s = [feature2_vals[imp] for imp in X_test['feature2']]\n",
    "        X_test = X_test.drop(columns=['feature2'])\n",
    "        X_test['feature2'] = encoded_feature2s\n",
    "    X_test = X_test[columns]\n",
    "    ids = X_test['feature0'].tolist()\n",
    "    X_test = X_test.drop(columns='feature0')\n",
    "    \n",
    "    # Make sure the data types are the same, because NumPy arrays are not tolerated in places where DataFrames are expected.\n",
    "    for col in X_test.columns:\n",
    "        X_test[col] = X_test[col].astype(X_train[col].dtype)\n",
    "\n",
    "    # Pre-process the test data.\n",
    "    X_test = transform(X_test, preprocessor)\n",
    "\n",
    "    # Pool the predictions into a list.\n",
    "    all_preds = []\n",
    "    for i in range(num_models):\n",
    "        preds = models[i].predict_proba(X_test)  \n",
    "        all_preds.append(preds)\n",
    "\n",
    "    # Apply the soft-voting function.\n",
    "    y_pred_proba = soft_vote(np.array(all_preds))\n",
    "    y_pred = [np.argmax(p) for p in y_pred_proba]\n",
    "\n",
    "    # Map the labels to numbers.\n",
    "    label_mapping = {'Neutral': 0, 'GOF': 1, 'LOF': 2}\n",
    "    y_test_numeric = [label_mapping[label] for label in y_test['label']]\n",
    "\n",
    "    # Compute class-wise MCC.\n",
    "    mcc_per_class = {}\n",
    "    for i, class_name in enumerate(['Neutral', 'GOF', 'LOF']):\n",
    "        y_true_binary = (np.array(y_test_numeric) == i).astype(int)\n",
    "        y_pred_binary = (np.array(y_pred) == i).astype(int)\n",
    "        mcc_per_class[class_name] = matthews_corrcoef(y_true_binary, y_pred_binary)\n",
    "\n",
    "    overall_mcc = matthews_corrcoef(y_test_numeric, y_pred)\n",
    "\n",
    "    print(\"Per-class MCC:\")\n",
    "    for class_name, mcc_value in mcc_per_class.items():\n",
    "        print(f\"{class_name}: {mcc_value:.4f}\")\n",
    "    print(f\"Overall MCC: {overall_mcc:.4f}\")\n",
    "    \n",
    "    def format_confusion_matrix(conf_matrix):\n",
    "        return \"\\n\".join([\",\".join(map(str, row)) for row in conf_matrix])\n",
    "\n",
    "    # Perform the evaluation using SciKit-learn's metrics.\n",
    "    accuracy = accuracy_score(y_test_numeric, y_pred)\n",
    "    precision = precision_score(y_test_numeric, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test_numeric, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test_numeric, y_pred, average='weighted')\n",
    "    roc_auc = roc_auc_score(y_test_numeric, y_pred_proba, multi_class='ovo')\n",
    "    conf_matrix = confusion_matrix(y_test_numeric, y_pred)\n",
    "\n",
    "    # Print the recorded metrics.\n",
    "    print(f'Model {number} - Test Accuracy: {accuracy}')\n",
    "    print(f'Model {number} - Test Precision: {precision}')\n",
    "    print(f'Model {number} - Test Recall: {recall}')\n",
    "    print(f'Model {number} - Test F1 Score: {f1}')\n",
    "    print(f'Model {number} - Test ROC AUC Score: {roc_auc}')\n",
    "\n",
    "    # Compute the remaining class-wise metrics.\n",
    "    report = classification_report(y_test_numeric, y_pred, target_names=['Neutral', 'GOF', 'LOF'])\n",
    "    print(report)\n",
    "    custom_report = classification_report(y_test_numeric, y_pred, target_names=['Neutral', 'GOF', 'LOF'])\n",
    "    custom_report += \"\\n\\nPer-class MCC:\\n\"\n",
    "    for class_name, mcc_value in mcc_per_class.items():\n",
    "        custom_report += f\"{class_name}: {mcc_value:.4f}\\n\"\n",
    "    print(custom_report)\n",
    "\n",
    "    # Export the metrics to a CSV.\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Model': [number],\n",
    "        'Accuracy': [accuracy],\n",
    "        'Precision': [precision],\n",
    "        'Recall': [recall],\n",
    "        'F1 Score': [f1],\n",
    "        'ROC AUC Score': [roc_auc],\n",
    "        'Overall MCC': [overall_mcc]\n",
    "    })\n",
    "    metrics_df.to_csv(metrics_path, index=False)\n",
    "    print(f\"Metrics for model {number} saved to {metrics_path}\")\n",
    "\n",
    "    # Export classification report to CSV.\n",
    "    report_df = pd.DataFrame({\n",
    "        'Classification Report': [custom_report]\n",
    "    })\n",
    "    report_df.to_csv(report_path, index=False, quoting=0)\n",
    "    print(f\"Report for model {number} saved to {report_path}\")\n",
    "\n",
    "    # Generate the confusion matrix.\n",
    "    matrix_df = pd.DataFrame(conf_matrix, \n",
    "                            columns=['Predicted Neutral', 'Predicted GOF', 'Predicted LOF'],\n",
    "                            index=['Actual Neutral', 'Actual GOF', 'Actual LOF'])\n",
    "\n",
    "    # Export matrix to CSV.\n",
    "    matrix_df.to_csv(matrix_path, quoting=0)\n",
    "    print(f\"Confusion Matrix for model {number} saved to {matrix_path}\")\n",
    "\n",
    "    out = []\n",
    "    for i in range(len(y_pred)):\n",
    "        out.append([ids[i], ['Neutral', 'GOF', 'LOF'][y_pred[i]], *y_pred_proba[i]])\n",
    "    out = pd.DataFrame(out, columns=['feature0', 'prediction', 'LoGoFunc_Neutral', 'LoGoFunc_GOF', 'LoGoFunc_LOF'])\n",
    "    out.to_csv(output_path, index=None)\n",
    "\n",
    "    print(f\"Results for model {number} saved to {output_path}\")\n",
    "    print(\"---------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../XGBoost/metrics/combined_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\" This combines all of the individual metrics into one file. \"\"\"\n",
    "\n",
    "metrics_dir = '../XGBoost/metrics/individual'\n",
    "\n",
    "# Init list to hold the df's.\n",
    "dfs = []\n",
    "\n",
    "# Iterate over every metric file.\n",
    "for i in range(1, 500):\n",
    "    file_path = os.path.join(metrics_dir, f'xgb_df{i}_metrics.csv')\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        dfs.append(df)\n",
    "\n",
    "# Combine the df's.\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Sort by the 'Model' column.\n",
    "combined_df = combined_df.sort_values('Model')\n",
    "combined_df.set_index('Model', inplace=True)\n",
    "\n",
    "# Save new CSV file.\n",
    "output_path = '../XGBoost/metrics/combined_metrics.csv'\n",
    "combined_df.to_csv(output_path)\n",
    "\n",
    "print(f\"Saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../XGBoost/metrics/ranked_by_accuracy.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\" This ranks the models according to accuracy. \"\"\"\n",
    "\n",
    "input_path = '../XGBoost/metrics/combined_metrics.csv'\n",
    "combined_df = pd.read_csv(input_path, index_col='Model')\n",
    "\n",
    "# Sort ACC from low to high.\n",
    "sorted_df = combined_df.sort_values('Accuracy', ascending=True)\n",
    "\n",
    "# New df only needs 'Model' and 'Accuracy' columns.\n",
    "rankings_df = pd.DataFrame({\n",
    "    'Model': sorted_df.index,\n",
    "    'Accuracy': sorted_df['Accuracy']\n",
    "})\n",
    "\n",
    "# Reset index so 'Model' is normal column.\n",
    "rankings_df = rankings_df.reset_index(drop=True)\n",
    "\n",
    "# Save new CSV file.\n",
    "output_path = '../XGBoost/metrics/ranked_by_accuracy.csv'\n",
    "rankings_df.to_csv(output_path, index=False)\n",
    "print(f\"Saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../XGBoost/metrics/ranked_by_precision.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\" This ranks the models according to precision. \"\"\"\n",
    "\n",
    "input_path = '../XGBoost/metrics/combined_metrics.csv'\n",
    "combined_df = pd.read_csv(input_path, index_col='Model')\n",
    "\n",
    "# Sort PREC from low to high.\n",
    "sorted_df = combined_df.sort_values('Precision', ascending=True)\n",
    "\n",
    "# New df only needs 'Model' and 'Precision' columns.\n",
    "rankings_df = pd.DataFrame({\n",
    "    'Model': sorted_df.index,\n",
    "    'Precision': sorted_df['Precision']\n",
    "})\n",
    "\n",
    "# Reset index so 'Model' is normal column.\n",
    "rankings_df = rankings_df.reset_index(drop=True)\n",
    "\n",
    "# Save new CSV file.\n",
    "output_path = '../XGBoost/metrics/ranked_by_precision.csv'\n",
    "rankings_df.to_csv(output_path, index=False)\n",
    "print(f\"Saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../XGBoost/metrics/ranked_by_recall.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\" This ranks the models according to recall. \"\"\"\n",
    "\n",
    "input_path = '../XGBoost/metrics/combined_metrics.csv'\n",
    "combined_df = pd.read_csv(input_path, index_col='Model')\n",
    "\n",
    "# Sort REC from low to high.\n",
    "sorted_df = combined_df.sort_values('Recall', ascending=True)\n",
    "\n",
    "# New df only needs 'Model' and 'Recall' columns.\n",
    "rankings_df = pd.DataFrame({\n",
    "    'Model': sorted_df.index,\n",
    "    'Recall': sorted_df['Recall']\n",
    "})\n",
    "\n",
    "# Reset index so 'Model' is normal column.\n",
    "rankings_df = rankings_df.reset_index(drop=True)\n",
    "\n",
    "# Save new CSV file.\n",
    "output_path = '../XGBoost/metrics/ranked_by_recall.csv'\n",
    "rankings_df.to_csv(output_path, index=False)\n",
    "print(f\"Saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../XGBoost/metrics/ranked_by_f1.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\" This ranks the models according to F1-score. \"\"\"\n",
    "\n",
    "input_path = '../XGBoost/metrics/combined_metrics.csv'\n",
    "combined_df = pd.read_csv(input_path, index_col='Model')\n",
    "\n",
    "# Sort F1 from low to high.\n",
    "sorted_df = combined_df.sort_values('F1 Score', ascending=True)\n",
    "\n",
    "# New df only needs 'Model' and 'F1 Score' columns.\n",
    "rankings_df = pd.DataFrame({\n",
    "    'Model': sorted_df.index,\n",
    "    'F1_Score': sorted_df['F1 Score']\n",
    "})\n",
    "\n",
    "# Reset index so 'Model' is normal column.\n",
    "rankings_df = rankings_df.reset_index(drop=True)\n",
    "\n",
    "# Save new CSV file.\n",
    "output_path = '../XGBoost/metrics/ranked_by_f1.csv'\n",
    "rankings_df.to_csv(output_path, index=False)\n",
    "print(f\"Saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../XGBoost/metrics/ranked_by_roc_auc.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\" This ranks the models according to ROC AUC. \"\"\"\n",
    "\n",
    "input_path = '../XGBoost/metrics/combined_metrics.csv'\n",
    "combined_df = pd.read_csv(input_path, index_col='Model')\n",
    "\n",
    "# ROC AUC Score from low to high.\n",
    "sorted_df = combined_df.sort_values('ROC AUC Score', ascending=True)\n",
    "\n",
    "# New df only needs 'Model' and 'ROC AUC' columns.\n",
    "rankings_df = pd.DataFrame({\n",
    "    'Model': sorted_df.index,\n",
    "    'ROC_AUC_Score': sorted_df['ROC AUC Score']\n",
    "})\n",
    "\n",
    "# Reset index so 'Model' is normal column.\n",
    "rankings_df = rankings_df.reset_index(drop=True)\n",
    "\n",
    "# Save new CSV.\n",
    "output_path = '../XGBoost/metrics/ranked_by_roc_auc.csv'\n",
    "rankings_df.to_csv(output_path, index=False)\n",
    "print(f\"Saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logofunc3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
