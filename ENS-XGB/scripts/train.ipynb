{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.special import softmax\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import  RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This loads all of the ENS ranked subsets and trains 499 models, one for each subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7b496cd46e453894f91d75ed47f87c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading CSV files:   0%|          | 0/499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 499 dataframes.\n",
      "dict_keys(['df1', 'df2', 'df3', 'df4', 'df5', 'df6', 'df7', 'df8', 'df9', 'df10', 'df11', 'df12', 'df13', 'df14', 'df15', 'df16', 'df17', 'df18', 'df19', 'df20', 'df21', 'df22', 'df23', 'df24', 'df25', 'df26', 'df27', 'df28', 'df29', 'df30', 'df31', 'df32', 'df33', 'df34', 'df35', 'df36', 'df37', 'df38', 'df39', 'df40', 'df41', 'df42', 'df43', 'df44', 'df45', 'df46', 'df47', 'df48', 'df49', 'df50', 'df51', 'df52', 'df53', 'df54', 'df55', 'df56', 'df57', 'df58', 'df59', 'df60', 'df61', 'df62', 'df63', 'df64', 'df65', 'df66', 'df67', 'df68', 'df69', 'df70', 'df71', 'df72', 'df73', 'df74', 'df75', 'df76', 'df77', 'df78', 'df79', 'df80', 'df81', 'df82', 'df83', 'df84', 'df85', 'df86', 'df87', 'df88', 'df89', 'df90', 'df91', 'df92', 'df93', 'df94', 'df95', 'df96', 'df97', 'df98', 'df99', 'df100', 'df101', 'df102', 'df103', 'df104', 'df105', 'df106', 'df107', 'df108', 'df109', 'df110', 'df111', 'df112', 'df113', 'df114', 'df115', 'df116', 'df117', 'df118', 'df119', 'df120', 'df121', 'df122', 'df123', 'df124', 'df125', 'df126', 'df127', 'df128', 'df129', 'df130', 'df131', 'df132', 'df133', 'df134', 'df135', 'df136', 'df137', 'df138', 'df139', 'df140', 'df141', 'df142', 'df143', 'df144', 'df145', 'df146', 'df147', 'df148', 'df149', 'df150', 'df151', 'df152', 'df153', 'df154', 'df155', 'df156', 'df157', 'df158', 'df159', 'df160', 'df161', 'df162', 'df163', 'df164', 'df165', 'df166', 'df167', 'df168', 'df169', 'df170', 'df171', 'df172', 'df173', 'df174', 'df175', 'df176', 'df177', 'df178', 'df179', 'df180', 'df181', 'df182', 'df183', 'df184', 'df185', 'df186', 'df187', 'df188', 'df189', 'df190', 'df191', 'df192', 'df193', 'df194', 'df195', 'df196', 'df197', 'df198', 'df199', 'df200', 'df201', 'df202', 'df203', 'df204', 'df205', 'df206', 'df207', 'df208', 'df209', 'df210', 'df211', 'df212', 'df213', 'df214', 'df215', 'df216', 'df217', 'df218', 'df219', 'df220', 'df221', 'df222', 'df223', 'df224', 'df225', 'df226', 'df227', 'df228', 'df229', 'df230', 'df231', 'df232', 'df233', 'df234', 'df235', 'df236', 'df237', 'df238', 'df239', 'df240', 'df241', 'df242', 'df243', 'df244', 'df245', 'df246', 'df247', 'df248', 'df249', 'df250', 'df251', 'df252', 'df253', 'df254', 'df255', 'df256', 'df257', 'df258', 'df259', 'df260', 'df261', 'df262', 'df263', 'df264', 'df265', 'df266', 'df267', 'df268', 'df269', 'df270', 'df271', 'df272', 'df273', 'df274', 'df275', 'df276', 'df277', 'df278', 'df279', 'df280', 'df281', 'df282', 'df283', 'df284', 'df285', 'df286', 'df287', 'df288', 'df289', 'df290', 'df291', 'df292', 'df293', 'df294', 'df295', 'df296', 'df297', 'df298', 'df299', 'df300', 'df301', 'df302', 'df303', 'df304', 'df305', 'df306', 'df307', 'df308', 'df309', 'df310', 'df311', 'df312', 'df313', 'df314', 'df315', 'df316', 'df317', 'df318', 'df319', 'df320', 'df321', 'df322', 'df323', 'df324', 'df325', 'df326', 'df327', 'df328', 'df329', 'df330', 'df331', 'df332', 'df333', 'df334', 'df335', 'df336', 'df337', 'df338', 'df339', 'df340', 'df341', 'df342', 'df343', 'df344', 'df345', 'df346', 'df347', 'df348', 'df349', 'df350', 'df351', 'df352', 'df353', 'df354', 'df355', 'df356', 'df357', 'df358', 'df359', 'df360', 'df361', 'df362', 'df363', 'df364', 'df365', 'df366', 'df367', 'df368', 'df369', 'df370', 'df371', 'df372', 'df373', 'df374', 'df375', 'df376', 'df377', 'df378', 'df379', 'df380', 'df381', 'df382', 'df383', 'df384', 'df385', 'df386', 'df387', 'df388', 'df389', 'df390', 'df391', 'df392', 'df393', 'df394', 'df395', 'df396', 'df397', 'df398', 'df399', 'df400', 'df401', 'df402', 'df403', 'df404', 'df405', 'df406', 'df407', 'df408', 'df409', 'df410', 'df411', 'df412', 'df413', 'df414', 'df415', 'df416', 'df417', 'df418', 'df419', 'df420', 'df421', 'df422', 'df423', 'df424', 'df425', 'df426', 'df427', 'df428', 'df429', 'df430', 'df431', 'df432', 'df433', 'df434', 'df435', 'df436', 'df437', 'df438', 'df439', 'df440', 'df441', 'df442', 'df443', 'df444', 'df445', 'df446', 'df447', 'df448', 'df449', 'df450', 'df451', 'df452', 'df453', 'df454', 'df455', 'df456', 'df457', 'df458', 'df459', 'df460', 'df461', 'df462', 'df463', 'df464', 'df465', 'df466', 'df467', 'df468', 'df469', 'df470', 'df471', 'df472', 'df473', 'df474', 'df475', 'df476', 'df477', 'df478', 'df479', 'df480', 'df481', 'df482', 'df483', 'df484', 'df485', 'df486', 'df487', 'df488', 'df489', 'df490', 'df491', 'df492', 'df493', 'df494', 'df495', 'df496', 'df497', 'df498', 'df499'])\n"
     ]
    }
   ],
   "source": [
    "# Points to the training data.\n",
    "train_dir = \"../data/encoded/train\"\n",
    "\n",
    "# Creates subset CSV list.\n",
    "csv_files = [f'X_train_id_top_{i}_features.csv' for i in range(1, 500)]\n",
    "\n",
    "# Init dictionary to store dataframes.\n",
    "dfs = {}\n",
    "\n",
    "# Iteratively load CSVs.\n",
    "for feature_number in tqdm(range(1, 500), desc=\"Loading CSV files\"):\n",
    "    df_name = f\"df{feature_number}\"\n",
    "    file_path = os.path.join(train_dir, f'X_train_{feature_number}_encoded.csv')\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "    dfs[df_name] = df\n",
    "\n",
    "print(f\"Loaded {len(dfs)} dataframes.\")\n",
    "print(dfs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best params for XGBoost.\n",
    "import json\n",
    "with open('top_27_params.json', 'r') as f:\n",
    "    top_n_params = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEGONE_FEATURES: ['feature369', 'feature265', 'feature473', 'feature420', 'feature461', 'feature415', 'feature373', 'feature433', 'feature395', 'feature443', 'feature242', 'feature258', 'feature451', 'feature366', 'feature425', 'feature46', 'feature31', 'feature6', 'feature89', 'feature52', 'feature468', 'feature286', 'feature315', 'feature191', 'feature196', 'feature91', 'feature192', 'feature27', 'feature20', 'feature111', 'feature100', 'feature90', 'feature72', 'feature255', 'feature298', 'feature50', 'feature67', 'feature136', 'feature59', 'feature92', 'feature139', 'feature60', 'feature48', 'feature145', 'feature78', 'feature115', 'feature372', 'feature472', 'feature439', 'feature382', 'feature475', 'feature353', 'feature134', 'feature132', 'feature35', 'feature42', 'feature96', 'feature249', 'feature82', 'feature18', 'feature23', 'feature123', 'feature99', 'feature32', 'feature70', 'feature68', 'feature178', 'feature401', 'feature141', 'feature294', 'feature447', 'feature95', 'feature375', 'feature55', 'feature421', 'feature56', 'feature34', 'feature304', 'feature77', 'feature435', 'feature365', 'feature226', 'feature360', 'feature347', 'feature193', 'feature314', 'feature299', 'feature403', 'feature402', 'feature334', 'feature350', 'feature110', 'feature271', 'feature229', 'feature337', 'feature268', 'feature328', 'feature293', 'feature223', 'feature85', 'feature361', 'feature140', 'feature297', 'feature254', 'feature340', 'feature306', 'feature172', 'feature155', 'feature240', 'feature236', 'feature239', 'feature209', 'feature238', 'feature187', 'feature364', 'feature330', 'feature437', 'feature159', 'feature441', 'feature431', 'feature396', 'feature358', 'feature295', 'feature444', 'feature316', 'feature380', 'feature75', 'feature448', 'feature205', 'feature430', 'feature148', 'feature335', 'feature326', 'feature308', 'feature320', 'feature318', 'feature389', 'feature276', 'feature305', 'feature296', 'feature390', 'feature204', 'feature127', 'feature54', 'feature408', 'feature376', 'feature413', 'feature282', 'feature338', 'feature356', 'feature385', 'feature283', 'feature245', 'feature93', 'feature362', 'feature352', 'feature264', 'feature272', 'feature333', 'feature384', 'feature86', 'feature302', 'feature429', 'feature231', 'feature152', 'feature174', 'feature349', 'feature322', 'feature379', 'feature300', 'feature252', 'feature464', 'feature200', 'feature164', 'feature194', 'feature197', 'feature218', 'feature368', 'feature423', 'feature339', 'feature387', 'feature374', 'feature391', 'feature181', 'feature311', 'feature427', 'feature213', 'feature125', 'feature184', 'feature290', 'feature284', 'feature317', 'feature406', 'feature237', 'feature281', 'feature449', 'feature310', 'feature248', 'feature177', 'feature359', 'feature394', 'feature291', 'feature416', 'feature371', 'feature327', 'feature336', 'feature355', 'feature312', 'feature182', 'feature367', 'feature325', 'feature203', 'feature263', 'feature412', 'feature214', 'feature219', 'feature377', 'feature138', 'feature79', 'feature109', 'feature195', 'feature454', 'feature405', 'feature201', 'feature463', 'feature432', 'feature210', 'feature465', 'feature484', 'feature397', 'feature381', 'feature456', 'feature462', 'feature288', 'feature438', 'feature275', 'feature383', 'feature363', 'feature212', 'feature158', 'feature392', 'feature445', 'feature498', 'feature494', 'feature492', 'feature478', 'feature493', 'feature469', 'feature489', 'feature188', 'feature168', 'feature418', 'feature118', 'feature404', 'feature167', 'feature113', 'feature142', 'feature107', 'feature147', 'feature157', 'feature257', 'feature103', 'feature114', 'feature94', 'feature189', 'feature112', 'feature76', 'feature260', 'feature161', 'feature230', 'feature143', 'feature83', 'feature175', 'feature162', 'feature105', 'feature58', 'feature227', 'feature73', 'feature228', 'feature222', 'feature351', 'feature388', 'feature126', 'feature206', 'feature440', 'feature292', 'feature146', 'feature341', 'feature303', 'feature307', 'feature321', 'feature243', 'feature287', 'feature149', 'feature128', 'feature51', 'feature69', 'feature190', 'feature221', 'feature428', 'feature124', 'feature133', 'feature235', 'feature344', 'feature217', 'feature483', 'feature487', 'feature477', 'feature488', 'feature474', 'feature453', 'feature471', 'feature499', 'feature496', 'feature497', 'feature495', 'feature393', 'feature476', 'feature479', 'feature470', 'feature457', 'feature485', 'feature446', 'feature486', 'feature481', 'feature490', 'feature480', 'feature491', 'feature323', 'feature399', 'feature244', 'feature482', 'feature400', 'feature466', 'feature436', 'feature411', 'feature121', 'feature153', 'feature173', 'feature80', 'feature154', 'feature62', 'feature150', 'feature186', 'feature279', 'feature57', 'feature280', 'feature176', 'feature116', 'feature269', 'feature37', 'feature65', 'feature108', 'feature250', 'feature259', 'feature53', 'feature64', 'feature234', 'feature39', 'feature104', 'feature97', 'feature165', 'feature313', 'feature459', 'feature442', 'feature261', 'feature407', 'feature342', 'feature169', 'feature151', 'feature33', 'feature117', 'feature266', 'feature426', 'feature331', 'feature424', 'feature370', 'feature422', 'feature343', 'feature285', 'feature357', 'feature274', 'feature256', 'feature409', 'feature267', 'feature417', 'feature273', 'feature233']\n",
      "MEDIAN_FEATURES: ['feature40', 'feature84', 'feature378', 'feature156', 'feature458', 'feature253', 'feature120', 'feature38', 'feature87', 'feature171', 'feature199', 'feature246', 'feature324', 'feature220', 'feature467', 'feature47', 'feature170', 'feature12', 'feature15', 'feature22', 'feature49', 'feature26', 'feature61', 'feature130', 'feature4', 'feature13', 'feature45', 'feature289', 'feature24', 'feature43', 'feature329', 'feature11', 'feature348', 'feature332', 'feature21', 'feature144', 'feature180', 'feature232', 'feature10', 'feature25', 'feature160', 'feature119', 'feature224', 'feature3', 'feature66', 'feature309', 'feature241', 'feature102', 'feature19', 'feature9', 'feature7', 'feature225', 'feature8', 'feature5', 'feature131', 'feature455', 'feature354', 'feature452', 'feature460', 'feature247', 'feature101', 'feature88', 'feature17', 'feature319', 'feature216', 'feature30', 'feature44', 'feature135', 'feature208', 'feature278', 'feature198', 'feature166', 'feature202', 'feature16', 'feature129', 'feature137', 'feature179', 'feature211', 'feature71', 'feature106', 'feature122', 'feature215', 'feature270', 'feature207', 'feature81', 'feature450', 'feature41', 'feature14', 'feature345', 'feature163', 'feature63', 'feature98', 'feature36', 'feature346', 'feature185', 'feature183', 'feature251', 'feature386', 'feature74', 'feature28']\n"
     ]
    }
   ],
   "source": [
    "# Load encoded NEGONE and MEDIAN feature lists.\n",
    "file_path = '../data/negone_median.json'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "negone_features = data.get('NEGONE_FEATURES', [])\n",
    "median_features = data.get('MEDIAN_FEATURES', [])\n",
    "\n",
    "print('NEGONE_FEATURES:', negone_features)\n",
    "print('MEDIAN_FEATURES:', median_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Keys: [6, 18, 23, 32, 42, 48, 50, 59, 60, 67, 68, 70, 78, 82, 89, 92, 96, 99, 115, 123, 136, 139, 145, 178, 249, 298]\n",
      "Length: 26\n"
     ]
    }
   ],
   "source": [
    "# Load other encoded feature names. \n",
    "\n",
    "file_path = '../data/patterns.json'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Get keys, convert to integer numbers.\n",
    "keys_list = [int(key.replace('feature', '')) for key in data.keys()]\n",
    "drop_numbers = keys_list\n",
    "print('Extracted Keys:', drop_numbers)\n",
    "print('Length:', len(drop_numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Iterate through the entire training script for each subeset DataFrame in the dictionary. \"\"\"\n",
    "\n",
    "train_dfs = dfs\n",
    "for df_name, X_train in train_dfs.items():\n",
    "    y_train = pd.read_csv('../data/y_train_id.csv')\n",
    "    print(y_train.shape)\n",
    "\n",
    "    # Encoding the labels.\n",
    "    y_train_enc = []\n",
    "    for lab in y_train['label']:\n",
    "        if lab == 'GOF':\n",
    "            y_train_enc.append(1)\n",
    "        elif lab == 'LOF':\n",
    "            y_train_enc.append(2)\n",
    "        else:\n",
    "            y_train_enc.append(0)\n",
    "    y_train = y_train_enc\n",
    "    print(len(y_train))\n",
    "\n",
    "    print(f\"Processing {df_name}\")\n",
    "\n",
    "    model_name = f\"xgb_{df_name}\"\n",
    "    model_directory = f'../XGBoost/models/{model_name}'\n",
    "\n",
    "    # Create the directory.\n",
    "    if not os.path.exists(model_directory):\n",
    "        os.makedirs(model_directory)\n",
    "\n",
    "    print(f\"Directory '{model_directory}' is ready.\")\n",
    "\n",
    "    # These lists are used to determine imputation strategies for the specified features (inherited from LoGoFunc)\n",
    "    NEGONE_FEATURES = ['feature369', 'feature265', 'feature473', 'feature420', 'feature461', 'feature415', 'feature373', 'feature433', 'feature395', 'feature443', 'feature242', 'feature258', 'feature451', 'feature366', 'feature425', 'feature46', 'feature31', 'feature6', 'feature89', 'feature52', 'feature468', 'feature286', 'feature315', 'feature191', 'feature196', 'feature91', 'feature192', 'feature27', 'feature20', 'feature111', 'feature100', 'feature90', 'feature72', 'feature255', 'feature298', 'feature50', 'feature67', 'feature136', 'feature59', 'feature92', 'feature139', 'feature60', 'feature48', 'feature145', 'feature78', 'feature115', 'feature372', 'feature472', 'feature439', 'feature382', 'feature475', 'feature353', 'feature134', 'feature132', 'feature35', 'feature42', 'feature96', 'feature249', 'feature82', 'feature18', 'feature23', 'feature123', 'feature99', 'feature32', 'feature70', 'feature68', 'feature178', 'feature401', 'feature141', 'feature294', 'feature447', 'feature95', 'feature375', 'feature55', 'feature421', 'feature56', 'feature34', 'feature304', 'feature77', 'feature435', 'feature365', 'feature226', 'feature360', 'feature347', 'feature193', 'feature314', 'feature299', 'feature403', 'feature402', 'feature334', 'feature350', 'feature110', 'feature271', 'feature229', 'feature337', 'feature268', 'feature328', 'feature293', 'feature223', 'feature85', 'feature361', 'feature140', 'feature297', 'feature254', 'feature340', 'feature306', 'feature172', 'feature155', 'feature240', 'feature236', 'feature239', 'feature209', 'feature238', 'feature187', 'feature364', 'feature330', 'feature437', 'feature159', 'feature441', 'feature431', 'feature396', 'feature358', 'feature295', 'feature444', 'feature316', 'feature380', 'feature75', 'feature448', 'feature205', 'feature430', 'feature148', 'feature335', 'feature326', 'feature308', 'feature320', 'feature318', 'feature389', 'feature276', 'feature305', 'feature296', 'feature390', 'feature204', 'feature127', 'feature54', 'feature408', 'feature376', 'feature413', 'feature282', 'feature338', 'feature356', 'feature385', 'feature283', 'feature245', 'feature93', 'feature362', 'feature352', 'feature264', 'feature272', 'feature333', 'feature384', 'feature86', 'feature302', 'feature429', 'feature231', 'feature152', 'feature174', 'feature349', 'feature322', 'feature379', 'feature300', 'feature252', 'feature464', 'feature200', 'feature164', 'feature194', 'feature197', 'feature218', 'feature368', 'feature423', 'feature339', 'feature387', 'feature374', 'feature391', 'feature181', 'feature311', 'feature427', 'feature213', 'feature125', 'feature184', 'feature290', 'feature284', 'feature317', 'feature406', 'feature237', 'feature281', 'feature449', 'feature310', 'feature248', 'feature177', 'feature359', 'feature394', 'feature291', 'feature416', 'feature371', 'feature327', 'feature336', 'feature355', 'feature312', 'feature182', 'feature367', 'feature325', 'feature203', 'feature263', 'feature412', 'feature214', 'feature219', 'feature377', 'feature138', 'feature79', 'feature109', 'feature195', 'feature454', 'feature405', 'feature201', 'feature463', 'feature432', 'feature210', 'feature465', 'feature484', 'feature397', 'feature381', 'feature456', 'feature462', 'feature288', 'feature438', 'feature275', 'feature383', 'feature363', 'feature212', 'feature158', 'feature392', 'feature445', 'feature498', 'feature494', 'feature492', 'feature478', 'feature493', 'feature469', 'feature489', 'feature188', 'feature168', 'feature418', 'feature118', 'feature404', 'feature167', 'feature113', 'feature142', 'feature107', 'feature147', 'feature157', 'feature257', 'feature103', 'feature114', 'feature94', 'feature189', 'feature112', 'feature76', 'feature260', 'feature161', 'feature230', 'feature143', 'feature83', 'feature175', 'feature162', 'feature105', 'feature58', 'feature227', 'feature73', 'feature228', 'feature222', 'feature351', 'feature388', 'feature126', 'feature206', 'feature440', 'feature292', 'feature146', 'feature341', 'feature303', 'feature307', 'feature321', 'feature243', 'feature287', 'feature149', 'feature128', 'feature51', 'feature69', 'feature190', 'feature221', 'feature428', 'feature124', 'feature133', 'feature235', 'feature344', 'feature217', 'feature483', 'feature487', 'feature477', 'feature488', 'feature474', 'feature453', 'feature471', 'feature499', 'feature496', 'feature497', 'feature495', 'feature393', 'feature476', 'feature479', 'feature470', 'feature457', 'feature485', 'feature446', 'feature486', 'feature481', 'feature490', 'feature480', 'feature491', 'feature323', 'feature399', 'feature244', 'feature482', 'feature400', 'feature466', 'feature436', 'feature411', 'feature121', 'feature153', 'feature173', 'feature80', 'feature154', 'feature62', 'feature150', 'feature186', 'feature279', 'feature57', 'feature280', 'feature176', 'feature116', 'feature269', 'feature37', 'feature65', 'feature108', 'feature250', 'feature259', 'feature53', 'feature64', 'feature234', 'feature39', 'feature104', 'feature97', 'feature165', 'feature313', 'feature459', 'feature442', 'feature261', 'feature407', 'feature342', 'feature169', 'feature151', 'feature33', 'feature117', 'feature266', 'feature426', 'feature331', 'feature424', 'feature370', 'feature422', 'feature343', 'feature285', 'feature357', 'feature274', 'feature256', 'feature409', 'feature267', 'feature417', 'feature273', 'feature233']\n",
    "    MEDIAN_FEATURES = ['feature40', 'feature84', 'feature378', 'feature156', 'feature458', 'feature253', 'feature120', 'feature38', 'feature87', 'feature171', 'feature199', 'feature246', 'feature324', 'feature220', 'feature467', 'feature47', 'feature170', 'feature12', 'feature15', 'feature22', 'feature49', 'feature26', 'feature61', 'feature130', 'feature4', 'feature13', 'feature45', 'feature289', 'feature24', 'feature43', 'feature329', 'feature11', 'feature348', 'feature332', 'feature21', 'feature144', 'feature180', 'feature232', 'feature10', 'feature25', 'feature160', 'feature119', 'feature224', 'feature3', 'feature66', 'feature309', 'feature241', 'feature102', 'feature19', 'feature9', 'feature7', 'feature225', 'feature8', 'feature5', 'feature131', 'feature455', 'feature354', 'feature452', 'feature460', 'feature247', 'feature101', 'feature88', 'feature17', 'feature319', 'feature216', 'feature30', 'feature44', 'feature135', 'feature208', 'feature278', 'feature198', 'feature166', 'feature202', 'feature16', 'feature129', 'feature137', 'feature179', 'feature211', 'feature71', 'feature106', 'feature122', 'feature215', 'feature270', 'feature207', 'feature81', 'feature450', 'feature41', 'feature14', 'feature345', 'feature163', 'feature63', 'feature98', 'feature36', 'feature346', 'feature185', 'feature183', 'feature251', 'feature386', 'feature74', 'feature28']\n",
    "\n",
    "    # Filter NEGONE_FEATURES and MEDIAN_FEATURES based on current X_train columns.  \n",
    "    negone_features_filtered = [feature for feature in NEGONE_FEATURES if feature in X_train.columns]\n",
    "    median_features_filtered = [feature for feature in MEDIAN_FEATURES if feature in X_train.columns]\n",
    "\n",
    "    # Redefine NEGONE_FEATURES and MEDIAN_FEATURES with updated values.\n",
    "    NEGONE_FEATURES = negone_features_filtered\n",
    "    MEDIAN_FEATURES = median_features_filtered\n",
    "\n",
    "    print(\"Filtered NEGONE_FEATURES:\", NEGONE_FEATURES)\n",
    "    print(\"Filtered MEDIAN_FEATURES:\", MEDIAN_FEATURES)\n",
    "\n",
    "    def generate_preprocessor(numeric_features, categorical_features, N_JOBS, cat_encode_type, \n",
    "                                do_specificimpute, do_featureselection, \n",
    "                                do_sampling, do_pca, var_thresh, oversample_technique, \n",
    "                                negone_features=NEGONE_FEATURES, median_features=MEDIAN_FEATURES,\n",
    "                                prefix='', do_feature_subset=False, max_features=1, do_removeppi=False, do_removegtex=False):\n",
    "        cat_encoders = [OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1, encoded_missing_value=-1), \n",
    "                        OneHotEncoder(sparse=False, handle_unknown='infrequent_if_exist', min_frequency=10)]\n",
    "        categorical_transformer = cat_encoders[cat_encode_type]\n",
    "\n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', MinMaxScaler(feature_range =(0, 1), clip=True))])\n",
    "\n",
    "        median_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', MinMaxScaler(feature_range =(0, 1), clip=True))])\n",
    "\n",
    "        negone_transformer = Pipeline(steps=[\n",
    "            ('scaler', MinMaxScaler(feature_range =(0, 1), clip=True)),\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value=-1)),\n",
    "        ])\n",
    "\n",
    "        preprocessor = None\n",
    "        if do_specificimpute:\n",
    "            preprocessor = ColumnTransformer(\n",
    "                transformers=[\n",
    "                    ('median', median_transformer, median_features),\n",
    "                    ('negone', negone_transformer, negone_features),\n",
    "                    ('cat', categorical_transformer, categorical_features),\n",
    "            ])\n",
    "        else:\n",
    "            preprocessor = ColumnTransformer(\n",
    "                transformers=[\n",
    "                    ('numeric', numeric_transformer, numeric_features),\n",
    "                    ('cat', categorical_transformer, categorical_features),\n",
    "                ])\n",
    "\n",
    "        vt = VarianceThreshold(threshold=var_thresh)\n",
    "        steps = [('initial', preprocessor), ('removeba', RemoveBeforeAfterTransformer()), ('variance_threshold', vt)]\n",
    "        if do_sampling == 1:\n",
    "            steps.append(('undersampling', RandomUnderSampler(random_state=42)))\n",
    "        if do_sampling == 2:\n",
    "            oversamplers = [SMOTE(n_jobs=N_JOBS,random_state=42), RandomOverSampler(random_state=42)]\n",
    "            steps.append(('oversampling', oversamplers[oversample_technique]))\n",
    "        if do_pca:\n",
    "            steps.append(('pca', PCA()))\n",
    "\n",
    "        preprocessor = Pipeline(steps=steps)\n",
    "        return preprocessor\n",
    "\n",
    "    class RemoveBeforeAfterTransformer(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self):\n",
    "            self.drop_cols = None\n",
    "\n",
    "        def fit(self, X, y=None):\n",
    "            print(X.shape)\n",
    "            self.drop_cols = [col for col in X.columns if any(f'feature{num}' == col for num in drop_numbers)]\n",
    "            print(f\"Columns to be dropped: {self.drop_cols}\")\n",
    "            return self\n",
    "\n",
    "        def transform(self, X, y=None):\n",
    "            print(f\"Columns being dropped: {[col for col in self.drop_cols if col in X.columns]}\")\n",
    "            X = X.drop(columns=self.drop_cols, errors='ignore')\n",
    "            print(X.shape)\n",
    "            return X\n",
    "\n",
    "        def get_feature_names_out(self, input_features=None):\n",
    "            return [f for f in input_features if f not in self.drop_cols]\n",
    "\n",
    "    def preprocess(preprocessor, train_data, train_labels, quiet=False):\n",
    "        for k, v in preprocessor.steps:\n",
    "            print(f\"Applying step: {k}\")\n",
    "            print(f\"Input shape: {train_data.shape}\")\n",
    "            print(f\"Input columns: {train_data.columns}\")\n",
    "            \n",
    "            if k == 'initial':\n",
    "                start = time.time()\n",
    "                v.fit(train_data)\n",
    "                train_data = pd.DataFrame(v.transform(train_data), columns=v.get_feature_names_out())\n",
    "                end = time.time()\n",
    "                if not quiet:\n",
    "                    print(k + ' took ' + str(end - start) + ' to run.')\n",
    "            elif k == 'oversampling' or k == 'undersampling':\n",
    "                start = time.time()\n",
    "                input_features = train_data.columns\n",
    "                train_data, train_labels = v.fit_resample(train_data, train_labels)\n",
    "                train_data = pd.DataFrame(train_data, columns=input_features)\n",
    "                end = time.time()\n",
    "                if not quiet:\n",
    "                    print(k + ' took ' + str(end - start) + ' to run.')\n",
    "            else:\n",
    "                start = time.time()\n",
    "                v.fit(train_data)\n",
    "                input_features = train_data.columns\n",
    "                train_data = pd.DataFrame(v.transform(train_data), columns=v.get_feature_names_out(input_features))\n",
    "                end = time.time()\n",
    "                if not quiet:\n",
    "                    print(k + ' took ' + str(end - start) + ' to run.')\n",
    "            \n",
    "            print(f\"Output shape: {train_data.shape}\")\n",
    "            print(f\"Output columns: {train_data.columns}\")\n",
    "            print(\"---\")\n",
    "\n",
    "        for col in train_data.columns:\n",
    "            try:\n",
    "                train_data[col] = train_data[col].astype('float')\n",
    "            except:\n",
    "                train_data[col] = train_data[col].astype('category')\n",
    "\n",
    "        return train_data, train_labels\n",
    "\n",
    "    def transform(test_data, preprocessor, quiet=False):\n",
    "        for k, v in preprocessor.steps:\n",
    "            if k == 'initial':\n",
    "                test_data = pd.DataFrame(v.transform(test_data), columns=v.get_feature_names_out())\n",
    "            elif k == 'oversampling' or k == 'undersampling':\n",
    "                continue\n",
    "            else:\n",
    "                test_data = v.transform(test_data)\n",
    "        test_data = pd.DataFrame(test_data)\n",
    "\n",
    "        for col in test_data.columns:\n",
    "            try:\n",
    "                test_data[col] = test_data[col].astype('float')\n",
    "            except:\n",
    "                test_data[col] = test_data[col].astype('category')\n",
    "\n",
    "        return test_data.to_numpy()\n",
    "\n",
    "    # Specify the paths for model, results, and metrics.\n",
    "    model_path = model_directory\n",
    "    output_path = f'../XGBoost/results/{model_name}-validation.csv'\n",
    "    metrics_file = f'../XGBoost//metrics/{model_name}-validation-metrics.csv'\n",
    "\n",
    "    # This is used to drop columns that only contain NaN values.\n",
    "    def drop_allnan(data):\n",
    "        for col in data.columns:\n",
    "            if data[col].isna().sum() == len(data):\n",
    "                data = data.drop(columns=col)\n",
    "        return data\n",
    "\n",
    "    # This encodes the IMPACT feature's column.\n",
    "    X_train = drop_allnan(X_train)\n",
    "    feature1_vals = {'LOW': 0, 'MODIFIER': 1, 'MODERATE': 1.5, 'HIGH': 2}\n",
    "\n",
    "    for col in X_train.columns:\n",
    "        if X_train[col].dtype == 'object':\n",
    "            if set(X_train[col].unique()) <= set(feature1_vals.keys()):\n",
    "                X_train[col] = X_train[col].map(feature1_vals).fillna(0)\n",
    "\n",
    "    # Conditional imputation based on feature type.\n",
    "    numeric_features = X_train.select_dtypes(include=['number']).columns\n",
    "    categorical_features = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # Pre-processor generation with multiple parameters that can be tweaked. I used the same values as LoGoFunc\n",
    "    preprocessor = generate_preprocessor(numeric_features, categorical_features, 40, 0,\n",
    "                                1, 0, 2, 0, 0, 1,\n",
    "                                prefix='light0', do_feature_subset=True, max_features=1)\n",
    "\n",
    "    if isinstance(y_train, list):\n",
    "        y_train = pd.DataFrame({'label': y_train})\n",
    "\n",
    "    X_train, y_train = preprocess(preprocessor, X_train, y_train)\n",
    "\n",
    "    # This creates a directory for the preprocessor and trained models.\n",
    "    joblib.dump(preprocessor, f'{model_path}/preprocessor.joblib')\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "    def train_model(params, X_train, y_train):\n",
    "        model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss', **params)\n",
    "        model.fit(X_train, y_train)\n",
    "        return model\n",
    "\n",
    "    # Train the ensemble in parallel.\n",
    "    models = Parallel(n_jobs=-1)(delayed(train_model)(params, X_train, y_train) for params in top_n_params)\n",
    "\n",
    "    # Export models and params.\n",
    "    for i, (model, params) in enumerate(zip(models, top_n_params)):\n",
    "        joblib.dump(model, f'{model_path}/xgb_model_{i}.joblib')\n",
    "        with open(f'{model_path}/xgb_params_{i}.json', 'w') as f:\n",
    "            json.dump(params, f, indent=2)\n",
    "\n",
    "    print(f\"Training complete for {df_name}.\")\n",
    "\n",
    "print(\"Finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logofunc3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
